{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from torch.functional import norm\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "import graph\n",
    "from graph.submission import save_models, compute_save_submission\n",
    "from graph.training import ultimate_unsupervised_training, ultimate_supervised_training, ultimate_unsupervised_training2, ultimate_supervised_training2\n",
    "from graph.gcn import GCN, GAT, GIN, GIN_shared_weights, GIN_GRU\n",
    "from graph.load_data import ultimate_dataloader, init_nodes_embedding \n",
    "from graph.classifier import Classifier_Dense, Classifier_RNN, style_Dense\n",
    "from graph.config import Config\n",
    "from graph.auc_loss import ROC_LOSS, ROC_STAR_LOSS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wandb\n",
    "log = False\n",
    "# Interactive display\n",
    "display= False\n",
    "# Save models and predictions\n",
    "save = False\n",
    "compute_predictions = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "\n",
    "    # Structure\n",
    "    \"gnn_type\" : GIN,\n",
    "    \"classifier_type\" : style_Dense,\n",
    "    \"depth_dense_block\": 1,\n",
    "\n",
    "    # Embedding dims and GCN\n",
    "    \"init_embedding_method\" : 'eigen2014',\n",
    "    \"gnn_hidden_dims\" : [24,32,32],\n",
    "\n",
    "    # Classifier\n",
    "    \"classifier_hidden_dims\" : [None, 32, 16, 2],\n",
    "    \"classifier_activation\" : \"Relu\",\n",
    "\n",
    "    \"unsupervised_training_years\" : [y for y in range(2011-5+1, 2011+1)],\n",
    "\n",
    "    # Config Unsupervised Training loader\n",
    "    \"minimal_degs_unsupervised\" : 5,\n",
    "\n",
    "    # Generator unsupervised\n",
    "    \"iter_number_unsupervised\" : 250,\n",
    "    \"max_neighbors_unsupervised\" : 5,\n",
    "    \"k_init\" : 1,\n",
    "    \"starting_size\" : 32,\n",
    "\n",
    "    # Unsupervised training\n",
    "    \"epochs_supervised\": 1,\n",
    "    \"batch_size_unsupervised\" : 1,\n",
    "\n",
    "    # Unsupervised optimizer\n",
    "    \"optimizer_algo_unsupervised\" : \"Adam\",\n",
    "    \"lr_unsupervised\": 5e-4,\n",
    "\n",
    "    # Config Supervised Training loader\n",
    "    \"delta_years\": 1,\n",
    "    \"min_year\": 2011-5,\n",
    "    \"batch_size_supervised\" : 1,\n",
    "    \"pairs_batch_size\": 2**15,\n",
    "    \"proportion_dataset\": 2,\n",
    "    \"minimal_degs_supervised\": 5,\n",
    "    \"test_rate\": 1,\n",
    "    \"max_size_dataset\":1_000_000//2,\n",
    "\n",
    "    # Supervised training\n",
    "    \"add_diag\": False,\n",
    "    \"normalize_adj_matrix\": True,\n",
    "    \"loss_fn\" : \"CrossEntropy\",\n",
    "    \"iter_number_supervised\": 300,\n",
    "\n",
    "    # Supervised optimizer\n",
    "    \"optimizer_algo_supervised_gnn\" : \"Adam\",\n",
    "    \"optimizer_algo_supervised_classifier\" : \"Adam\",\n",
    "    \"lr_supervised_gnn\": 1e-3,\n",
    "    \"lr_supervised_classifier\": 1e-3,\n",
    "    \n",
    "    \"drop\": 0.3,\n",
    "    \"batch_norm\": False,\n",
    "\n",
    "\n",
    "    \"early_stop\":False\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "config = Config(config)\n",
    "\n",
    "depth = len(config.gnn_hidden_dims) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if log:\n",
    "    wandb.init(project=\"HyperparametersTesting\", entity=\"argocs\", config=config)\n",
    "    wandb.define_metric(\"unsupervised step\")\n",
    "    wandb.define_metric(\"supervised step\")\n",
    "    wandb.define_metric(\"loss GNN\", step_metric=\"unsupervised step\")\n",
    "    wandb.define_metric(\"train AUC\", step_metric=\"supervised step\")\n",
    "    wandb.define_metric(\"test AUC\", step_metric=\"supervised step\")\n",
    "    wandb.define_metric(\"train loss classifier\", step_metric=\"supervised step\")\n",
    "    wandb.define_metric(\"test loss classifier\", step_metric=\"supervised step\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph for year 2006 has 106391 edges\n",
      "Graph for year 2007 has 141614 edges\n",
      "Graph for year 2008 has 192874 edges\n",
      "Graph for year 2009 has 278820 edges\n",
      "Graph for year 2010 has 427201 edges\n",
      "Graph for year 2011 has 651997 edges\n",
      "Graph for year 2012 has 1042310 edges\n",
      "Graph for year 2013 has 1582114 edges\n",
      "Graph for year 2014 has 2278611 edges\n",
      "Graph for year 2015 has 3342061 edges\n",
      "Graph for year 2016 has 5018339 edges\n",
      "Graph for year 2017 has 7652945 edges\n",
      "Ratio links:  0.33457075758220584\n",
      "Ratio links:  0.33464769715749354\n",
      "Ratio links:  0.3346030271752818\n",
      "Ratio links:  0.33473731811560564\n",
      "Ratio links:  0.33469842938362576\n",
      "Ratio links:  0.3347151565440057\n",
      "Ratio links:  0.33479405376269483\n",
      "Ratio links:  0.33492510787508106\n",
      "Ratio links:  0.33516523108332613\n"
     ]
    }
   ],
   "source": [
    "loader = ultimate_dataloader(   min_year=config.min_year,\n",
    "                                proportion_dataset=config.proportion_dataset,\n",
    "                                minimal_degs_unsupervised=config.minimal_degs_unsupervised,\n",
    "                                minimal_degs_supervised=config.minimal_degs_supervised,\n",
    "                                add_diag=config.add_diag,\n",
    "                                normalize_adj_matrix=config.normalize_adj_matrix,\n",
    "                                max_size=config.max_size_dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph for year 2006 has 106391 edges\n",
      "Graph for year 2007 has 141614 edges\n",
      "Graph for year 2008 has 192874 edges\n",
      "Graph for year 2009 has 278820 edges\n",
      "Graph for year 2010 has 427201 edges\n",
      "Graph for year 2011 has 651997 edges\n",
      "Graph for year 2012 has 1042310 edges\n",
      "Graph for year 2013 has 1582114 edges\n",
      "Graph for year 2014 has 2278611 edges\n",
      "Graph for year 2014 has 2278611 edges\n"
     ]
    }
   ],
   "source": [
    "\n",
    "graph_sparse, _, _, _ = graph.data_utils.load_data('CompetitionSet2017_3.pkl')\n",
    "# -- extract every graph\n",
    "graphs_years = [graph.data_utils.extract_graph(graph_sparse, year) for year in range(config.min_year, 2014 + 1)]\n",
    "\n",
    "initial_embedding = init_nodes_embedding(config.init_embedding_method, config.gnn_hidden_dims[0], graphs_years, config.minimal_degs_unsupervised, normalize=True).to(graph.device)\n",
    "\n",
    "del graph_sparse\n",
    "del graphs_years\n",
    "del _\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn = config.gnn_type(config.gnn_hidden_dims, config.depth_dense_block).to(graph.device)\n",
    "\n",
    "# Optimizer GNN\n",
    "unsupervised_optimizer = torch.optim.Adam(gnn.parameters(), lr=config.lr_unsupervised)\n",
    "\n",
    "\n",
    "input_dim_classifier = config.gnn_hidden_dims[-1] * config.delta_years * 2\n",
    "config.classifier_hidden_dims[0] = input_dim_classifier\n",
    "\n",
    "if config.classifier_activation == \"Relu\":\n",
    "    activation_type = torch.nn.ReLU\n",
    "elif config.classifier_activation == \"PRelu\":\n",
    "    activation_type = torch.nn.PReLU\n",
    "elif config.classifier_activation == \"Gelu\":\n",
    "    activation_type = torch.nn.GELU\n",
    "else:\n",
    "    print(f\"The following activation is not implemented: {config.classifier_activation}\")\n",
    "    exit()\n",
    "\n",
    "if config.classifier_type == Classifier_Dense:\n",
    "\n",
    "    classifier = config.classifier_type(config.classifier_hidden_dims, activation_type).to(graph.device)\n",
    "    \n",
    "if config.classifier_type == Classifier_RNN:\n",
    "    classifier = config.classifier_type(config.classifier_hidden_dims, activation_type, config.gnn_hidden_dims[-1]).to(graph.device)\n",
    "    \n",
    "if config.classifier_type == style_Dense:\n",
    "    classifier = config.classifier_type(config.classifier_hidden_dims, activation_type, config.drop, config.batch_norm).to(graph.device)\n",
    "\n",
    "# Loss function\n",
    "if config.loss_fn == \"CrossEntropy\":\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "elif config.loss_fn == \"ROC_loss\":\n",
    "    loss_fn = ROC_LOSS(2048, 2048)\n",
    "elif config.loss_fn == \"ROC_star_loss\":\n",
    "    loss_fn = ROC_STAR_LOSS(2048, 2048, 0.4)\n",
    "\n",
    "# Optimizer classification\n",
    "optimizer_gnn = torch.optim.Adam(gnn.parameters(), lr=config.lr_supervised_gnn)\n",
    "optimizer_classifier = torch.optim.Adam(classifier.parameters(), lr=config.lr_supervised_classifier)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "if config.loss_fn == \"CrossEntropy\":\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "elif config.loss_fn == \"ROC_loss\":\n",
    "    loss_fn = ROC_LOSS(1024, 1024)\n",
    "elif config.loss_fn == \"ROC_star_loss\":\n",
    "    loss_fn = ROC_STAR_LOSS(2048, 2048, 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ultimate_unsupervised_training2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-de5af94509e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m ultimate_unsupervised_training2( epochs=1,\n\u001b[0m\u001b[1;32m      3\u001b[0m                                 \u001b[0mloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                 \u001b[0minitial_embedding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_embedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgnn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ultimate_unsupervised_training2' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "ultimate_unsupervised_training2( epochs=1,\n",
    "                                loader=loader,\n",
    "                                initial_embedding=initial_embedding,\n",
    "                                model=gnn,\n",
    "                                optimizer=optimizer_gnn,\n",
    "                                batch_size=config.batch_size_unsupervised,\n",
    "                                iter_number=config.iter_number_unsupervised,\n",
    "                                max_neighbors=config.max_neighbors_unsupervised,\n",
    "                                k_init=config.k_init,\n",
    "                                k_max=depth,\n",
    "                                starting_size=config.starting_size,\n",
    "                                display=display,\n",
    "                                log=log,\n",
    "                                years=config.unsupervised_training_years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######## Epoch 0 ########\n",
      "\n",
      "    [ 1  ]  Loss: 0.69410658   AUC: 0.51229151   from 2011 to 2014\n",
      "Loss: 0.68349308   AUC: 0.81308704   from 2014 to 2017\n",
      "    [ 2  ]  Loss: 0.68877310   AUC: 0.53289972   from 2011 to 2014\n",
      "Loss: 0.67321855   AUC: 0.85914926   from 2014 to 2017\n",
      "    [ 3  ]  Loss: 0.68438685   AUC: 0.54892944   from 2011 to 2014\n",
      "Loss: 0.66377193   AUC: 0.86531341   from 2014 to 2017\n",
      "    [ 4  ]  Loss: 0.68021560   AUC: 0.56665866   from 2011 to 2014\n",
      "Loss: 0.65433103   AUC: 0.86649889   from 2014 to 2017\n",
      "    [ 5  ]  Loss: 0.67619848   AUC: 0.58017877   from 2011 to 2014\n",
      "Loss: 0.64486492   AUC: 0.86625695   from 2014 to 2017\n",
      "    [ 6  ]  Loss: 0.67241079   AUC: 0.60284591   from 2011 to 2014\n",
      "Loss: 0.63670367   AUC: 0.86564395   from 2014 to 2017\n",
      "    [ 7  ]  Loss: 0.66966504   AUC: 0.60935301   from 2011 to 2014\n",
      "Loss: 0.63013721   AUC: 0.86544890   from 2014 to 2017\n",
      "    [ 8  ]  Loss: 0.66746700   AUC: 0.61629646   from 2011 to 2014\n",
      "Loss: 0.62410414   AUC: 0.86466252   from 2014 to 2017\n",
      "    [ 9  ]  Loss: 0.66413122   AUC: 0.63016969   from 2011 to 2014\n",
      "Loss: 0.61851579   AUC: 0.86305775   from 2014 to 2017\n",
      "    [ 10 ]  Loss: 0.66121513   AUC: 0.64917097   from 2011 to 2014\n",
      "Loss: 0.61325902   AUC: 0.86199568   from 2014 to 2017\n",
      "    [ 11 ]  Loss: 0.65867686   AUC: 0.66281715   from 2011 to 2014\n",
      "Loss: 0.60826582   AUC: 0.86035473   from 2014 to 2017\n",
      "    [ 12 ]  Loss: 0.65625757   AUC: 0.66798071   from 2011 to 2014\n",
      "Loss: 0.60338241   AUC: 0.85905552   from 2014 to 2017\n",
      "    [ 13 ]  Loss: 0.65260100   AUC: 0.68870989   from 2011 to 2014\n",
      "Loss: 0.59828699   AUC: 0.85845884   from 2014 to 2017\n",
      "    [ 14 ]  Loss: 0.65088844   AUC: 0.68950391   from 2011 to 2014\n",
      "Loss: 0.59305364   AUC: 0.85760787   from 2014 to 2017\n",
      "    [ 15 ]  Loss: 0.64665115   AUC: 0.70443809   from 2011 to 2014\n",
      "Loss: 0.58723789   AUC: 0.85660204   from 2014 to 2017\n",
      "    [ 16 ]  Loss: 0.64399296   AUC: 0.70928845   from 2011 to 2014\n",
      "Loss: 0.58105171   AUC: 0.85558552   from 2014 to 2017\n",
      "    [ 17 ]  Loss: 0.64071923   AUC: 0.71691497   from 2011 to 2014\n",
      "Loss: 0.57452357   AUC: 0.85419098   from 2014 to 2017\n",
      "    [ 18 ]  Loss: 0.63740265   AUC: 0.72250930   from 2011 to 2014\n",
      "Loss: 0.56781864   AUC: 0.85265657   from 2014 to 2017\n",
      "    [ 19 ]  Loss: 0.63355410   AUC: 0.72456801   from 2011 to 2014\n",
      "Loss: 0.56106120   AUC: 0.84986737   from 2014 to 2017\n",
      "    [ 20 ]  Loss: 0.62972963   AUC: 0.73518470   from 2011 to 2014\n",
      "Loss: 0.55442238   AUC: 0.84797596   from 2014 to 2017\n",
      "    [ 21 ]  Loss: 0.62762308   AUC: 0.73342516   from 2011 to 2014\n",
      "Loss: 0.54818600   AUC: 0.84680114   from 2014 to 2017\n",
      "    [ 22 ]  Loss: 0.62355232   AUC: 0.74073452   from 2011 to 2014\n",
      "Loss: 0.54234660   AUC: 0.84516839   from 2014 to 2017\n",
      "    [ 23 ]  Loss: 0.62102062   AUC: 0.74252822   from 2011 to 2014\n",
      "Loss: 0.53666520   AUC: 0.84468853   from 2014 to 2017\n",
      "    [ 24 ]  Loss: 0.61853391   AUC: 0.75059458   from 2011 to 2014\n",
      "Loss: 0.53137237   AUC: 0.84610713   from 2014 to 2017\n",
      "    [ 25 ]  Loss: 0.61426020   AUC: 0.75570731   from 2011 to 2014\n",
      "Loss: 0.52606243   AUC: 0.84795872   from 2014 to 2017\n",
      "    [ 26 ]  Loss: 0.60879010   AUC: 0.76801721   from 2011 to 2014\n",
      "Loss: 0.52049166   AUC: 0.84978384   from 2014 to 2017\n",
      "    [ 27 ]  Loss: 0.60858673   AUC: 0.76251787   from 2011 to 2014\n",
      "Loss: 0.51466447   AUC: 0.85199323   from 2014 to 2017\n",
      "    [ 28 ]  Loss: 0.60478437   AUC: 0.76458108   from 2011 to 2014\n",
      "Loss: 0.50810021   AUC: 0.85348113   from 2014 to 2017\n",
      "    [ 29 ]  Loss: 0.59916514   AUC: 0.77585298   from 2011 to 2014\n",
      "Loss: 0.50083703   AUC: 0.85410667   from 2014 to 2017\n",
      "    [ 30 ]  Loss: 0.59382904   AUC: 0.77768689   from 2011 to 2014\n",
      "Loss: 0.49326077   AUC: 0.85513242   from 2014 to 2017\n",
      "    [ 31 ]  Loss: 0.59197462   AUC: 0.77752847   from 2011 to 2014\n",
      "Loss: 0.48639113   AUC: 0.85654513   from 2014 to 2017\n",
      "    [ 32 ]  Loss: 0.58916068   AUC: 0.77842139   from 2011 to 2014\n",
      "Loss: 0.48049003   AUC: 0.85809249   from 2014 to 2017\n",
      "    [ 33 ]  Loss: 0.58616942   AUC: 0.77914124   from 2011 to 2014\n",
      "Loss: 0.47505966   AUC: 0.85975504   from 2014 to 2017\n",
      "    [ 34 ]  Loss: 0.58284146   AUC: 0.78459134   from 2011 to 2014\n",
      "Loss: 0.46993825   AUC: 0.86033614   from 2014 to 2017\n",
      "    [ 35 ]  Loss: 0.57692862   AUC: 0.79451385   from 2011 to 2014\n",
      "Loss: 0.46396750   AUC: 0.86078728   from 2014 to 2017\n",
      "    [ 36 ]  Loss: 0.57482445   AUC: 0.78843157   from 2011 to 2014\n",
      "Loss: 0.45659262   AUC: 0.86119234   from 2014 to 2017\n",
      "    [ 37 ]  Loss: 0.57008970   AUC: 0.79394183   from 2011 to 2014\n",
      "Loss: 0.44758460   AUC: 0.86139586   from 2014 to 2017\n",
      "    [ 38 ]  Loss: 0.56295633   AUC: 0.80054546   from 2011 to 2014\n",
      "Loss: 0.43937543   AUC: 0.86159631   from 2014 to 2017\n",
      "    [ 39 ]  Loss: 0.56262577   AUC: 0.79484308   from 2011 to 2014\n",
      "Loss: 0.43290910   AUC: 0.86153306   from 2014 to 2017\n",
      "    [ 40 ]  Loss: 0.55563748   AUC: 0.80468065   from 2011 to 2014\n",
      "Loss: 0.42853457   AUC: 0.86139404   from 2014 to 2017\n",
      "    [ 41 ]  Loss: 0.55276334   AUC: 0.80240014   from 2011 to 2014\n",
      "Loss: 0.42402899   AUC: 0.86110511   from 2014 to 2017\n",
      "    [ 42 ]  Loss: 0.54967767   AUC: 0.80580499   from 2011 to 2014\n",
      "Loss: 0.41808626   AUC: 0.86088417   from 2014 to 2017\n",
      "    [ 43 ]  Loss: 0.54403353   AUC: 0.80813234   from 2011 to 2014\n",
      "Loss: 0.40843382   AUC: 0.86096351   from 2014 to 2017\n",
      "    [ 44 ]  Loss: 0.53937715   AUC: 0.80943141   from 2011 to 2014\n",
      "Loss: 0.39918992   AUC: 0.86116625   from 2014 to 2017\n",
      "    [ 45 ]  Loss: 0.53527117   AUC: 0.81256422   from 2011 to 2014\n",
      "Loss: 0.39153856   AUC: 0.86119653   from 2014 to 2017\n",
      "\n",
      "######## Epoch 1 ########\n",
      "\n",
      "    [ 46 ]  Loss: 0.53312528   AUC: 0.81002741   from 2011 to 2014\n",
      "Loss: 0.38772446   AUC: 0.86106754   from 2014 to 2017\n",
      "    [ 47 ]  Loss: 0.52689093   AUC: 0.81416641   from 2011 to 2014\n",
      "Loss: 0.38551909   AUC: 0.86061496   from 2014 to 2017\n",
      "    [ 48 ]  Loss: 0.52260989   AUC: 0.81530679   from 2011 to 2014\n",
      "Loss: 0.37648177   AUC: 0.86089641   from 2014 to 2017\n",
      "    [ 49 ]  Loss: 0.51926285   AUC: 0.81982640   from 2011 to 2014\n",
      "Loss: 0.36819115   AUC: 0.86112257   from 2014 to 2017\n",
      "    [ 50 ]  Loss: 0.51662898   AUC: 0.81722926   from 2011 to 2014\n",
      "Loss: 0.36349428   AUC: 0.86118253   from 2014 to 2017\n",
      "    [ 51 ]  Loss: 0.51116359   AUC: 0.82114534   from 2011 to 2014\n",
      "Loss: 0.35780725   AUC: 0.86122335   from 2014 to 2017\n",
      "    [ 52 ]  Loss: 0.50337684   AUC: 0.82682885   from 2011 to 2014\n",
      "Loss: 0.35007754   AUC: 0.86144111   from 2014 to 2017\n",
      "    [ 53 ]  Loss: 0.50538862   AUC: 0.81890941   from 2011 to 2014\n",
      "Loss: 0.34508261   AUC: 0.86147636   from 2014 to 2017\n",
      "    [ 54 ]  Loss: 0.50389540   AUC: 0.81986906   from 2011 to 2014\n",
      "Loss: 0.34349507   AUC: 0.86129212   from 2014 to 2017\n",
      "    [ 55 ]  Loss: 0.50012964   AUC: 0.82063103   from 2011 to 2014\n",
      "Loss: 0.33642197   AUC: 0.86160311   from 2014 to 2017\n",
      "    [ 56 ]  Loss: 0.49545923   AUC: 0.82373749   from 2011 to 2014\n",
      "Loss: 0.32814097   AUC: 0.86199571   from 2014 to 2017\n",
      "    [ 57 ]  Loss: 0.49547735   AUC: 0.82089227   from 2011 to 2014\n",
      "Loss: 0.32358757   AUC: 0.86207859   from 2014 to 2017\n",
      "    [ 58 ]  Loss: 0.48807579   AUC: 0.82774860   from 2011 to 2014\n",
      "Loss: 0.32545832   AUC: 0.86173256   from 2014 to 2017\n",
      "    [ 59 ]  Loss: 0.48441893   AUC: 0.82851946   from 2011 to 2014\n",
      "Loss: 0.31779897   AUC: 0.86202219   from 2014 to 2017\n",
      "    [ 60 ]  Loss: 0.48469895   AUC: 0.82802711   from 2011 to 2014\n",
      "Loss: 0.30720535   AUC: 0.86245349   from 2014 to 2017\n",
      "    [ 61 ]  Loss: 0.48038474   AUC: 0.82939388   from 2011 to 2014\n",
      "Loss: 0.29969972   AUC: 0.86265486   from 2014 to 2017\n",
      "    [ 62 ]  Loss: 0.47749636   AUC: 0.82803082   from 2011 to 2014\n",
      "Loss: 0.30380070   AUC: 0.86229436   from 2014 to 2017\n",
      "    [ 63 ]  Loss: 0.48168066   AUC: 0.82593318   from 2011 to 2014\n",
      "Loss: 0.30959818   AUC: 0.86190310   from 2014 to 2017\n",
      "    [ 64 ]  Loss: 0.47875407   AUC: 0.82791973   from 2011 to 2014\n",
      "Loss: 0.30429736   AUC: 0.86223746   from 2014 to 2017\n",
      "    [ 65 ]  Loss: 0.47436023   AUC: 0.83115068   from 2011 to 2014\n",
      "Loss: 0.28521240   AUC: 0.86314590   from 2014 to 2017\n",
      "    [ 66 ]  Loss: 0.47052276   AUC: 0.82959149   from 2011 to 2014\n",
      "Loss: 0.27824748   AUC: 0.86350746   from 2014 to 2017\n",
      "    [ 67 ]  Loss: 0.47518748   AUC: 0.82529038   from 2011 to 2014\n"
     ]
    }
   ],
   "source": [
    "ultimate_supervised_training2(   epochs=10,\n",
    "                                loader=loader,\n",
    "                                initial_embedding=initial_embedding,\n",
    "                                gnn=gnn,\n",
    "                                classifier=classifier,\n",
    "                                optimizer_gnn=optimizer_gnn,\n",
    "                                optimizer_classifier=optimizer_classifier,\n",
    "                                loss_fn=loss_fn,\n",
    "                                pairs_batch_size=config.pairs_batch_size,\n",
    "                                batch_size=config.batch_size_supervised,\n",
    "                                delta_years=config.delta_years,\n",
    "                                test_rate=config.test_rate,\n",
    "                                log=log,\n",
    "                                early_stop=config.early_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-14-6d87f4e48e3a>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-6d87f4e48e3a>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Loss: 0.02038186   AUC: 0.82470053   from 2014 to 2017\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "######## Epoch 9 ########\n",
    "\n",
    "    [406 ]  Loss: 0.38940576   AUC: 0.87545571   from 2011 to 2014\n",
    "Loss: 0.28202456   AUC: 0.86608686   from 2014 to 2017\n",
    "    [407 ]  Loss: 0.38752237   AUC: 0.87595749   from 2011 to 2014\n",
    "Loss: 0.23246711   AUC: 0.86577067   from 2014 to 2017\n",
    "    [408 ]  Loss: 0.38071540   AUC: 0.87857586   from 2011 to 2014\n",
    "Loss: 0.22305581   AUC: 0.86580216   from 2014 to 2017\n",
    "    [409 ]  Loss: 0.38408196   AUC: 0.87695855   from 2011 to 2014\n",
    "Loss: 0.26694751   AUC: 0.86610463   from 2014 to 2017\n",
    "    [410 ]  Loss: 0.38202500   AUC: 0.87561480   from 2011 to 2014\n",
    "Loss: 0.28722411   AUC: 0.86627575   from 2014 to 2017\n",
    "    [411 ]  Loss: 0.38520452   AUC: 0.87501928   from 2011 to 2014\n",
    "Loss: 0.25047794   AUC: 0.86600841   from 2014 to 2017\n",
    "    [412 ]  Loss: 0.37661877   AUC: 0.88143097   from 2011 to 2014\n",
    "Loss: 0.22454691   AUC: 0.86572004   from 2014 to 2017\n",
    "    [413 ]  Loss: 0.37827235   AUC: 0.88111980   from 2011 to 2014\n",
    "Loss: 0.24380083   AUC: 0.86577940   from 2014 to 2017\n",
    "    [414 ]  Loss: 0.38138583   AUC: 0.87787096   from 2011 to 2014\n",
    "Loss: 0.27157584   AUC: 0.86598785   from 2014 to 2017\n",
    "    [415 ]  Loss: 0.37969592   AUC: 0.87884873   from 2011 to 2014\n",
    "Loss: 0.25880554   AUC: 0.86581884   from 2014 to 2017\n",
    "    [416 ]  Loss: 0.37899423   AUC: 0.87980500   from 2011 to 2014\n",
    "Loss: 0.23837371   AUC: 0.86575702   from 2014 to 2017\n",
    "    [417 ]  Loss: 0.38393492   AUC: 0.87721140   from 2011 to 2014\n",
    "Loss: 0.23906098   AUC: 0.86578760   from 2014 to 2017\n",
    "    [418 ]  Loss: 0.37971771   AUC: 0.87968952   from 2011 to 2014\n",
    "Loss: 0.26338875   AUC: 0.86584039   from 2014 to 2017\n",
    "    [419 ]  Loss: 0.37687910   AUC: 0.88092934   from 2011 to 2014\n",
    "Loss: 0.26886997   AUC: 0.86584609   from 2014 to 2017\n",
    "    [420 ]  Loss: 0.37981567   AUC: 0.87996631   from 2011 to 2014\n",
    "Loss: 0.24891324   AUC: 0.86584995   from 2014 to 2017\n",
    "    [421 ]  Loss: 0.37758258   AUC: 0.87985499   from 2011 to 2014\n",
    "Loss: 0.24172667   AUC: 0.86576143   from 2014 to 2017\n",
    "    [422 ]  Loss: 0.37794462   AUC: 0.87984030   from 2011 to 2014\n",
    "Loss: 0.25602397   AUC: 0.86576759   from 2014 to 2017\n",
    "    [423 ]  Loss: 0.37453911   AUC: 0.88156152   from 2011 to 2014\n",
    "Loss: 0.25556752   AUC: 0.86565790   from 2014 to 2017\n",
    "    [424 ]  Loss: 0.37681270   AUC: 0.88018798   from 2011 to 2014\n",
    "Loss: 0.24654341   AUC: 0.86561310   from 2014 to 2017\n",
    "    [425 ]  Loss: 0.38100138   AUC: 0.87853309   from 2011 to 2014\n",
    "Loss: 0.25414518   AUC: 0.86561591   from 2014 to 2017\n",
    "    [426 ]  Loss: 0.38046166   AUC: 0.87708491   from 2011 to 2014\n",
    "Loss: 0.25612000   AUC: 0.86560556   from 2014 to 2017\n",
    "    [427 ]  Loss: 0.38164866   AUC: 0.87641780   from 2011 to 2014\n",
    "Loss: 0.25474119   AUC: 0.86560742   from 2014 to 2017\n",
    "    [428 ]  Loss: 0.37588879   AUC: 0.88129129   from 2011 to 2014\n",
    "Loss: 0.25717217   AUC: 0.86555495   from 2014 to 2017\n",
    "    [429 ]  Loss: 0.37848443   AUC: 0.87969525   from 2011 to 2014\n",
    "Loss: 0.25201192   AUC: 0.86550903   from 2014 to 2017\n",
    "    [430 ]  Loss: 0.37927601   AUC: 0.87830079   from 2011 to 2014\n",
    "Loss: 0.25405392   AUC: 0.86549656   from 2014 to 2017\n",
    "    [431 ]  Loss: 0.37859219   AUC: 0.87976378   from 2011 to 2014\n",
    "Loss: 0.25582281   AUC: 0.86554405   from 2014 to 2017\n",
    "    [432 ]  Loss: 0.37450203   AUC: 0.88224845   from 2011 to 2014\n",
    "Loss: 0.25676119   AUC: 0.86555555   from 2014 to 2017\n",
    "    [433 ]  Loss: 0.37878054   AUC: 0.87922063   from 2011 to 2014\n",
    "Loss: 0.25017780   AUC: 0.86548981   from 2014 to 2017\n",
    "    [434 ]  Loss: 0.37575665   AUC: 0.88288118   from 2011 to 2014\n",
    "Loss: 0.26532903   AUC: 0.86560644   from 2014 to 2017\n",
    "    [435 ]  Loss: 0.37301591   AUC: 0.88343982   from 2011 to 2014\n",
    "Loss: 0.24702546   AUC: 0.86549374   from 2014 to 2017\n",
    "    [436 ]  Loss: 0.37733170   AUC: 0.88078200   from 2011 to 2014\n",
    "Loss: 0.24853857   AUC: 0.86546214   from 2014 to 2017\n",
    "    [437 ]  Loss: 0.37963885   AUC: 0.88015505   from 2011 to 2014\n",
    "Loss: 0.27275607   AUC: 0.86547809   from 2014 to 2017\n",
    "    [438 ]  Loss: 0.38045532   AUC: 0.87830018   from 2011 to 2014\n",
    "Loss: 0.26119304   AUC: 0.86548477   from 2014 to 2017\n",
    "    [439 ]  Loss: 0.37723398   AUC: 0.87921944   from 2011 to 2014\n",
    "Loss: 0.23649520   AUC: 0.86546606   from 2014 to 2017\n",
    "    [440 ]  Loss: 0.37882456   AUC: 0.88185787   from 2011 to 2014\n",
    "Loss: 0.28253663   AUC: 0.86556429   from 2014 to 2017\n",
    "    [441 ]  Loss: 0.37908894   AUC: 0.87951339   from 2011 to 2014\n",
    "Loss: 0.25570810   AUC: 0.86559075   from 2014 to 2017\n",
    "    [442 ]  Loss: 0.37679118   AUC: 0.88092705   from 2011 to 2014\n",
    "Loss: 0.24622680   AUC: 0.86553669   from 2014 to 2017\n",
    "    [443 ]  Loss: 0.37788200   AUC: 0.87887339   from 2011 to 2014\n",
    "Loss: 0.25996459   AUC: 0.86550344   from 2014 to 2017\n",
    "    [444 ]  Loss: 0.37553257   AUC: 0.88115480   from 2011 to 2014\n",
    "Loss: 0.26508048   AUC: 0.86552361   from 2014 to 2017\n",
    "    [445 ]  Loss: 0.37949449   AUC: 0.87935699   from 2011 to 2014\n",
    "Loss: 0.25547317   AUC: 0.86547976   from 2014 to 2017\n",
    "    [446 ]  Loss: 0.37540254   AUC: 0.88181100   from 2011 to 2014\n",
    "Loss: 0.25042370   AUC: 0.86549248   from 2014 to 2017\n",
    "    [447 ]  Loss: 0.37682334   AUC: 0.88031912   from 2011 to 2014\n",
    "Loss: 0.26584384   AUC: 0.86547431   from 2014 to 2017\n",
    "    [448 ]  Loss: 0.37305185   AUC: 0.88203347   from 2011 to 2014\n",
    "Loss: 0.26846296   AUC: 0.86562339   from 2014 to 2017\n",
    "    [449 ]  Loss: 0.37082076   AUC: 0.88348329   from 2011 to 2014\n",
    "Loss: 0.24273875   AUC: 0.86560457   from 2014 to 2017\n",
    "    [450 ]  Loss: 0.37719601   AUC: 0.88256487   from 2011 to 2014\n",
    "Loss: 0.26250067   AUC: 0.86572425   from 2014 to 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rng_state = np.random.get_state()\n",
    "loader.test_dataset[0] = np.random.permutation(loader.test_dataset[0])\n",
    "np.random.set_state(rng_state)\n",
    "loader.test_dataset[1] = np.random.permutation(loader.test_dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "c = len(os.listdir(\"Models\"))\n",
    "\n",
    "if save:\n",
    "    save_models(initial_embedding, gnn, classifier, name=c)\n",
    "\n",
    "if compute_predictions:\n",
    "    compute_save_submission(initial_embedding, gnn, classifier, config.delta_years, 2, filename=f\"Models/{c}/preds.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d580e719627d887212d2a19ca3306e3953175739e461f82f09c6b3cfa9708ce2"
  },
  "kernelspec": {
   "display_name": "envname",
   "language": "python",
   "name": "envname"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
